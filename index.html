<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="">
  <meta name="keywords" content="MathVista, Math Vista">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title> PRISM-Physics: Causal DAG-Based Process Evaluation for Physics Reasoning</title>

  <link rel="icon" href="./static/icon/prism.png">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="stylesheet" href="./static/css/leaderboard.css"> -->

  <!-- <link href="https://unpkg.com/tabulator-tables@5.5.2/dist/css/tabulator_bulma.min.css" rel="stylesheet">
  <script type="text/javascript" src="https://unpkg.com/tabulator-tables@5.5.2/dist/js/tabulator.min.js"></script> -->
  <script type="text/javascript" src="static/js/sort-table.js" defer></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <!-- <script src="./static/js/explorer-index.js"></script> -->
  <script src="./static/js/question_card.js"></script>

  <!-- <script src="./static/js/leaderboard_testmini.js"></script>   -->
  <script src="./data/results/output_folders.js" defer></script>
  <script src="./data/results/model_scores.js" defer></script>

  <script src="./visualizer/data/data_public.js" defer></script>
</head>
<body>



<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://wanjiazhao1203.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

       <a class="navbar-item" href="https://aquahorsem.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <a class="navbar-item" href="https://jingzheshi.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title is-bold">
            <img src="static/icon/prism.png" style="width:1.5em;vertical-align: middle" alt="Logo"/>
            <span class="mathvista" style="vertical-align: middle">PRISM-Physics</span>
            </h1>
          <h2 class="subtitle is-3 publication-subtitle">
            PRISM-Physics: Causal DAG-Based Process Evaluation for Physics Reasoning
            <!-- <br> -->
            <!-- with GPT-4V, Bard, and Other Large Multimodal Models -->
          </h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="">Wanjia Zhao*</a><sup style="color:#8C1515;">1</sup>,</span>
            <span class="author-block">
              <a href="">Qinwei Ma*</a><sup style="color:#671372;">2</sup>,</span>
            <span class="author-block">
              <a href="">Jingzhe Shi*</a><sup style="color:#671372;">2</sup>,</span>
            <span class="author-block">
              <a href="https://eleanor-h.github.io/">Shirley Wu</a><sup style="color:#8C1515;">1</sup>,</span>
            <span class="author-block">
              <a href="https://github.com/Nosiopd">Jiaqi Han</a><sup style="color:#8C1515;">1</sup>,</span>
            <span class="author-block">
              <a href="">Yijia Xiao</a><sup style="color:#2774AE;">3</sup>,</span>
            <span class="author-block">
              <a href="">Si-Yuan Chen</a><sup style="color:#ed1b34;">4</sup>,</span>
            <span class="author-block">
              <a href="">Xiao Luo</a><sup style="color:#c5050c;">5</sup>,</span>
            <span class="author-block">
              <a href="">Ludwig Schmidt</a><sup style="color:#8C1515;">1</sup>,</span>   
            <span class="author-block">
              <a href="">James Zou</a><sup style="color:#8C1515;">1</sup>,</span>  
            </span>            
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup style="color:#8C1515;">1</sup>Stanford University,</span>
            <span class="author-block"><sup style="color:#671372;">2</sup>Tsinghua University,</span><br>
            <span class="author-block"><sup style="color:#2774AE;">3</sup>UCLA,</span>
            <span class="author-block"><sup style="color:#ed1b34;">4</sup>Harvard University</span>
            <span class="author-block"><sup style="color: #c5050c;;">5</sup>University of Wisconsin-Madison </span><br>
          </div>
        
         
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2510.03185"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->

              <span class="link-block">
                <a href="https://open-prism.github.io/PRISM-Physics/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              
              <!-- Dataset Link (GitHub). -->
                            
              <!-- TODO: Dataset Link (HuggingFace). -->
              <span class="link-block">
                <a href="https://open-prism.github.io/PRISM-Physics/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <p style="font-size:18px">ü§ó</p>
                  </span>
                  <span>Dataset</span>
                </a>
              </span>

              <span class="link-block">
                <a href="#leaderboard"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <p style="font-size:18px">üèÜ</p>
                  </span>
                  <span>Leaderboard</span>
                </a>
              </span>


              <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <p style="font-size:18px">üöÄ</p>
                  </span>
                  <span>Challenge</span>
                </a>
              </span> -->

              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <p style="font-size:18px">üåê</p>
                  </span>
                  <span>Twitter</span>
                  </a>
              </span>

              <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <p style="font-size:18px">ü¶ã</p>
                  </span>
                  <span>Bluesky</span>
                  </a>
              </span> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p>
            We present <span class="mathvista">PRISM-Physics</span>, a process-level evaluation framework and benchmark for complex physics reasoning problems. Solutions are represented as directed acyclic graphs (DAGs) of formulas, explicitly encoding causal dependencies among intermediate steps to enable fine-grained, interpretable, and theoretically grounded scoring. 
            We prove the optimality of the DAG representation and the corresponding scoring policy. Combining with a fully rule-based method for symbolic formula equivalence matching that we developed, we ensure consistent validation across diverse formulations without heuristic judgments. Results show that our evaluation framework is more aligned with human experts' scoring. 
           Experiments on state-of-the-art LLMs reveal persistent reasoning failures in physics, while step-level scoring offers both diagnostic insight and rich signals for later training. By combining structural rigor, theoretical guarantees, and symbolic validation, \dataset provides a principled foundation for advancing process-level evaluation and guiding the development of models with deeper scientific reasoning capabilities.
          </p>          
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
</div>
</section>

<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <!-- <div class="column is-full-width has-text-centered"> -->
        <div class="column is-four-fifths">
        <div class="box m-5">
          <div class="content has-text-centered">
            <img src="static/images/overall.jpg" alt="Prism-Physics" width="80%"/>
            <p> Step-level Accuracy, Final-Answer Accuracy and response time for evaluated models.
          </div>
        </div>
      </div> 
      </div>

</section>


<!-- DAG SECTION -->
<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">

  <h1 class="title is-1 mathvista" id="leaderboard">
    <img src="static/icon/prism.png" style="width:1em;vertical-align: middle" alt="Logo"/>
    <span class="mathvista" style="vertical-align: middle">PRISM-DAG</span>
  </h1>
  </div>
</section>


<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/dag.jpg" alt="Prism-Physics" width="100%"/>
              <p> A data example with the proposed DAG structure.
            </div>
          </div>
          <div class="content has-text-justified">
            <p>
            ‚Ä¢	<strong>DAG Representation of Solutions</strong>
            Encode each solution as a Directed Acyclic Graph (DAG), where nodes are formulas and edges capture prerequisite relations. This provides a minimal, complete, and interpretable structure of the reasoning process. <br>
            ‚Ä¢	<strong>Ancestor Closure Scoring</strong>
            Score = matched formulas + all their prerequisites. This ensures fair credit propagation along causal chains, avoiding both over- and under-crediting. <br>
            ‚Ä¢	<strong>Optimality Guarantee</strong>
            We prove the optimality of the DAG representation and the corresponding scoring policy, ensuring no information loss and no redundant complexity.
            </p>
          </div>
        <h2 class="title is-3">Evaluation Framework</h2>
         <h2 class="title is-4">Rule-based Physics Formula Equivalence Matching</h2>
          <div class="content has-text-justified">
            <p>
              <strong>[Stage 1] Constant Substitution.</strong> We substitute certain variables with their expressions. Variables, constants, and units are normalized into predefined form for consistency. <br>

              <strong>[Stage 2] Solution Set Equivalence Check.</strong> For two equations with $N$ variables, one variable is randomly chosen as the target, the remaining $N-1$ are assigned random values, then and the target is solved to compare whether the solution sets are equivalent. This process is repeated for multiple iterations. Solution set equivalence serves as a proxy for equation equivalence.            </p>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/scoring_example.jpg" alt="Prism-Physics" width="80%"/>
              <p> An example of our scoring pipeline. A) Formula matching aligns student and reference formulas. B) Back-propagation grading highlights correctly credited formulas along the dependency DAG. C) The final score is computed as the sum of credited points, yielding <em>90/100</em> in this case.
            </div>
          </div>
          <h2 class="title is-4">Scoring Pipeline</h2>
          <div class="content has-text-justified">
            <p>
              <strong>Formula Extraction and Normalization.</strong> <br>
                ‚Ä¢	Given a student's solution, all mathematical expressions are first extracted and rewritten into our dataset's standardized canonical form, discarding invalid expressions such as syntactically malformed formulas or irrelevant numerical fragments.<br>

              <strong>Formula Matching.</strong>  <br>
              ‚Ä¢	Each standardized student formula is compared against the reference DAG of the solution according to Section~\ref{sec:matching}, which outputs a set of matched formulas in the DAG.<br>

              <strong>Scoring.</strong> <br>
              ‚Ä¢	Finally, we score the student solution according to the Ancestor Closure Scoring Policy in Section~\ref{sec:ancestor} with the DAG and the set of matched formulas.

                  <!-- Step 1 -->
                  <!-- <div style="flex:1; min-width:250px; border:1px solid #ccc; border-radius:8px; padding:8px; background:#f9fafc;">
                    <h3 style="margin-top:0;">1. <strong>Formula Extraction & Normalization</strong></h3>
                    <p>Extract mathematical expressions from the student solution and rewrite them into a standardized canonical form, discarding malformed or irrelevant fragments.</p>
                  </div> -->
                  
                  <!-- Step 2 -->
                  <!-- <div style="flex:1; min-width:250px; border:1px solid #ccc; border-radius:8px; padding:8px; background:#f9fafc;">
                    <h3 style="margin-top:0;">2. <strong>Formula Matching</strong></h3>
                    <p>Compare normalized student formulas with the reference DAG to identify matched formulas in the solution graph.</p>
                  </div> -->
                  
                  <!-- Step 3 -->
                  <!-- <div style="flex:1; min-width:250px; border:1px solid #ccc; border-radius:8px; padding:8px; background:#f9fafc;">
                    <h3 style="margin-top:0;">3. <strong>Scoring</strong></h3>
                    <p>Apply the <em>Ancestor Closure Scoring Policy</em>: credit matched formulas plus all their prerequisites in the DAG.</p>
                  </div> -->

  </div>
</section>
            </p>
          </div>
      </div>
    </div>
  </div>
</section>

	
<!-- DATASET SECTION -->
<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
    <!-- <h1 class="title is-1 mathvista"><img src="static/images/mathverse.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>MathVista Dataset</h1> -->
  <h1 class="title is-1 mathvista">
    <img src="static/icon/prism.png" style="width:1em;vertical-align: middle" alt="Logo"/>
    <span class="mathvista" style="vertical-align: middle">PRISM-Physics Dataset</span>
  </h1>
  </div>
</section>


<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Data Collection and Preprocessing</h2>
        <div class="box m-5">
          <div class="content has-text-centered">
            <img src="static/images/clean_data.jpg" alt="Prism-Physics" width="100%"/>
            <p> Overview of <b>Three-Step Rewriting Pipeline.</b>. 
          </div>
        </div>
        <div class="content has-text-justified">
          <p>
              <strong>Three-Step Rewriting Pipeline.</strong> To guarantee both internal consistency and external evaluability, every sample in the dataset is processed through a structured three-stage rewriting pipeline. Each stage focuses on eliminating ambiguity and enforcing standardization, while preserving the fidelity of the original content. <br>
              <strong>Verification and Quality Control.</strong> At each stage, an LLM-based module verifies formatting, clarity, and dependency rules; failures trigger corrective feedback and regeneration. <br>
              <strong>Fine-Grained Enhancements.</strong> Beyond the main pipeline, we applied several refinements: enforcing significant-figure rules, explicitly defining all constants and variables, and unifying answer formatting. <br>
              <br>
            You can download the dataset on <a href="" target="_blank">Hugging Face Dataset</a>.
          </p>
        </div>
      </div> 
    </div>
        

    <span style="margin:25pt">
    </span>

    <!-- <div class="columns is-centered m-6"> -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
        <h2 class="title is-3">Dataset statistics</h2>
        <div class="content has-text-justified">
          <div class="box m-5">
            <div class="content has-text-left">
              <p>
                <img src="static/images/pie.jpg" alt="Logo" 
                    style="float:left; margin:0 1.5em 1em 0; width:16em;">
                <strong>Difficulty Annotation.</strong>  <br>
                Each problem is assigned a composite difficulty label that integrates LLM-based ratings of conceptual depth and computational burden with an entropy-based DAG complexity measure. The three components are combined into a unified score, which is mapped to <em>Easy</em>, <em>Medium</em>, <em>Hard</em>, capturing both the content difficulty and the reasoning complexity of the solution.  <br>
                <strong>Physics Domain Categorization.</strong>  <br>
                    Each problem is categorized into one of seven key physics domains: <br>
                    (1) Mechanics, 
                    (2) Electromagnetism, 
                    (3) Optics, 
                    (4) Atomic, Nuclear, and Particle Physics,   <br>
                    (5) Thermodynamics and Statistical Physics, 
                    (6) Quantum Mechanics,
                    (7) Solid State Physics and Miscellaneous Topics.
              </p>
            </div>
          </div>

        </div>
      </div>
    </div>
  <!-- </div> -->

    

</section>

<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
  <h1 class="title is-1 mathvista" id="leaderboard">
    <img src="static/icon/prism.png" style="width:1em;vertical-align: middle" alt="Logo"/>
    <span class="mathvista" style="vertical-align: middle">Experiment Results</span>
  </h1>
  </div>
</section>

<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
          <h2 class="title is-3">Main Results </h2>
          <div id="results-carousel" class="carousel results-carousel">
            <div class="box m-5">
              <div class="content has-text-centered">
                <p>Table 1. Step-level Accuracy and Final-Answer Accuracy across difficulty levels (Easy, Medium, Hard, and Avg.) for evaluated models.</p>
                <img src="static/images/leaderboard.png" alt="" width="100%"/>
              </div>
            </div>
          </div>
          <h2 class="title is-4">Step-level vs. Answer-level Evaluation.</h2>

          <div class="content has-text-justified">  
            <p>
              Table 1 reports step-level and final-answer accuracy across difficulty levels. As problem difficulty rises, performance declines and response time increases, reflecting LLMs' sensitivity to longer reasoning chains, more demanding modeling, and higher computational effort. Final-answer and step-level evaluations diverge sharply with problem difficulty: final-answer accuracy drops by over 40% from easy to medium and below 10% on hard problems, while step-level scoring reveals that models still earn partial credit by applying key principles or deriving valid intermediate equations before failing at later stages.<br>

              These results demonstrate that final-answer scoring alone severely underestimates reasoning ability, whereas step-level evaluation provides a more faithful measure of process competence under complex tasks. Moreover, step-level signals open promising avenues for training and data curation: If evaluation relies solely on final answers, rewards on difficult problems become extremely sparse. Instead, step-level scoring provides <strong>rich intermediate reward signals</strong>, offering valuable guidance for reinforcement learning and a principled basis for constructing higher-quality training data.
            </p>
          </div>
          <div id="results-carousel" class="carousel results-carousel">
              <h2 class="title is-4">Physics Domain Category Analysis.</h2>

            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="static/images/category_acc.jpg" alt="" width="100%"/>
                <p>Step-level and final-answer accuracy across Physics Domain Categories and Difficulty Levels.</p>

              </div>
            </div>
          </div>
          <div class="content has-text-justified">  
            <p>
=              We analyze LLM performance across physics domains and difficulty levels, as shown in Figure. 
              Models exhibit varying accuracy across different types, with the highest performance observed in Thermodynamics and Statistical Physics and the lowest in Quantum Mechanics. Step-level evaluation further exposes weaknesses in reasoning coherence, and accuracy consistently drops from Easy to Hard problems across all domains.          </p>
          </div>
        </div>
      </div>
  </section>
    

    
<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Modality and Reasoning-Level Comparisons</h2>
          <h2 class="title is-4">Text Models vs. Multimodal Models</h2>
            <div class="content has-text-justified">  
              <p>
                The effect of multimodal input varies across model families. In general, adding images provides stronger gains at the step level than at the final-answer level, highlighting its role in supporting intermediate reasoning. However, for smaller or weaker models, multimodal input can even be detrimental, as diagrams in physics problems often serve a presentational rather than informational role, with the critical content already conveyed in text.        </div>
            <div id="results-carousel" class="carousel results-carousel">
          <h2 class="title is-4">Across Different Reasoning Level.</h2>
          <div class="content has-text-justified">

          

          <div class="box m-5">
            <div class="content has-text-left">
              <p>
                <img src="static/images/reasoning.jpg" alt="Logo" 
                    style="float:left; margin:0 1.5em 1em 0; width:28em;">
                As shown in Figure 4., we observe that reasoning-oriented models exhibit consistently higher accuracy than chat-oriented models, but this improvement consistently comes with substantially longer response times.
                We further evaluate GPT-5 and GPT-5-mini for three reasoning modes (<em>low</em>, <em>medium</em>, <em>high</em>). Results indicate a consistent improvement in accuracy with increasing reasoning effort. However, for GPT-5, the average latency of the <em>medium</em> mode is 83.38% higher than the <em>low</em> mode, while the <em>high</em> mode is 268.18% higher. GPT-5-mini shows the same pattern. These results confirm that deeper reasoning consistently improves accuracy while incurring proportional increases in computational cost. Notably, while o4-mini was previously claimed to be a good reasoning model, its performance here is relatively poor; one possible explanation is that, as a distilled model, it suffers from limited generalization and thus struggles with complex reasoning tasks beyond its training distribution.
              </p>
            </div>
          </div>

        </div>

        </div>
      </div>
    </div>
  </section>

  <section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Error Analysis</h2>
            <div id="results-carousel" class="carousel results-carousel">

            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="static/images/errors.jpg" alt="" width="60%"/>
                <p>Figure 5: Distribution of primary error types across models.</p>

              </div>
            </div>
          </div>
          <div class="content has-text-justified">  
            <p>
             We perform error analysis on the first incorrect step detected in each solution as shown in Figure 5, using a unified taxonomy that integrates process-level physics reasoning errors with formula-level derivation errors.  
            The classification covers seven categories:  
            (1) Diagram Analysis Error (DAE),  
            (2) Physics Theorem Application Error (PTAE),  
            (3) Modeling and Process Understanding Error (MPUE),  
            (4) Condition or Assumption Error (CAE),
            (5) Variable Relationship Error (VRE),  
            (6) Derivation and Computation Error (DCE), and  
            (7) Unit Dimension Error (UDE).<br>

            The dominant error types across models are Condition/Assumption Errors (CAE), which arise when models set up inconsistent or incorrect physical assumptions; Derivation & Computation Errors (DCE), which occur when models make mistakes in algebraic manipulation or calculation; and Modeling & Process Understanding Errors (MPUE), which reflect failures in mapping the problem into the correct physical model or reasoning process. This indicates that LLMs often fail both in establishing consistent physical conditions and in executing algebraic reasoning.          </div>
          
        </div>
      </div>
    </div>
  </section>



    <section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Evaluation Framework Analysis</h2>
          <h2 class="title is-4">Annotation Setup.</h2>
            <div class="content has-text-justified">  
              <p>
              We randomly sampled 70 problems (10 from each domain) along with their corresponding DeepSeek-V3 (text-only) solutions. Each problem‚Äìsolution pair was independently evaluated by two human experts to reduce variance, with annotators including an IPhO Gold Medalist and Top-Tier Physics PhD. 
                In cases where the two experts' scores differed substantially, a third annotator was invited to adjudicate and determine the final score.          
            </div>
            <div id="results-carousel" class="carousel results-carousel">
            <h2 class="title is-4">Results.</h2>
            <div class="content has-text-justified">  
                <p>
                We quantified the agreement between framework-generated scores and human annotations using Kendall's &tau;<sub>b</sub> correlation coefficient, along with statistical significance testing via both asymptotic and permutation-based p-values. Higher $\tau_b$ values indicate stronger concordance, with significance levels verifying the robustness of the observed correlations.            </div>
            <div id="results-carousel" class="carousel results-carousel">
            <div class="box m-5">
              <div class="content has-text-centered">
                <p>Table 2: Comparison of annotation alignment.</p>

                <img src="static/images/framework_analysis.png" alt="" width="50%"/>

              </div>
            </div>
      
            <div class="content has-text-justified">  
                <p>
                  Table 2 demonstrates the clear superiority of PRISM-DAG.
                  which achieves the highest &tau;<sub>b</sub> and lowest $p$-values. 
                  <em>LLM-as-Judge</em> is purely outcome-based, assigning only binary 0/1 scores, 
                  while <em>PSAS-S</em>, though process-based, evaluates steps independently 
                  without modeling causal dependencies. Both baselines are LLM-based, 
                  whereas our non-LLM PRISM-DAG explicitly accounts for causality across steps, 
                  leading to stronger alignment with human judgments. We analyzed failure cases from our evaluator and two baselines to understand strengths and limitations.             
                </p>
            </div>
            <div id="results-carousel" class="carousel results-carousel">

          </div>
  
      </div>
    </div>
  </section>






<!-- BIBTEX SECTION -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-3 has-text-centered">BibTeX</h2>
    <pre><code>
    @article{huang2024physics,
      title={PRISM-Physics: Causal DAG-Based Process Evaluation for Physics Reasoning},
      author={Wanjia Zhao, Qinwei Ma, Jingzhe Shi, Shirley Wu, Jiaqi Han, Yijia Xiao, Si-Yuan Chen, Xiao Luo, Ludwig Schmidt, James Zou},
      journal={arXiv preprint arXiv:2510.03185},
      year={2025}
    }
    </code></pre>
  </div>
</section>


<!-- COLLABORATORS SECTION -->
<section>
  <div class="section" id="org-banners" style="display:flex">
   <a href="https://www.stanford.edu/" target="_blank" rel="external">
        <div id="logo"><img src="static/logos/stanford.png" style="width:9.0em;vertical-align: middle"></div>
    </a>
    &emsp;&emsp;
    <a href="https://www.tsinghua.edu.cn/en/" target="_blank" rel="external">
        <div id="logo"><img src="static/logos/Tsinghua.png" style="width:11.0em;vertical-align: middle"></div>
    </a>     
    &emsp;&emsp;
    <a href="https://www.ucla.edu/" target="_blank" rel="external">
      <div id="logo"><img src="static/logos/ucla.png" style="width:7.0em;vertical-align: middle"></div>
    </a>   
    &emsp;&emsp;
    <a href="https://www.harvard.edu/" target="_blank" rel="external">
        <div id="logo"><img src="static/logos/harvard.png" style="width:10.0em;vertical-align: middle"></div>
    </a>  
    &emsp;&emsp;
    <a href="https://www.wisc.edu/" target="_blank" rel="external">
        <div id="logo"><img src="static/logos/wisc.png" style="width:10.0em;vertical-align: middle"></div>
    </a>  
  </div>
</section>


<!-- FOOTER SECTION -->
<footer class="footer">
  <!-- <div class="container"> -->
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is adapted from <a href="https://fveler.github.io/">FVELer</a>, <a href="https://mathverse-cuhk.github.io/">MathVerse</a>, <a href="https://mathvista.github.io/">MathVista</a>, and <a href="https://nerfies.github.io/">Nerfies</a>, licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  <!-- </div> -->
</footer>

</body>
</html>
